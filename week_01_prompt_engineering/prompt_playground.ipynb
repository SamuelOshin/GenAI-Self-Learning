{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9344367b",
   "metadata": {},
   "source": [
    "# Prompt Playground\n",
    "\n",
    "This notebook provides a structured environment to experiment with different prompt engineering techniques for Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8a1bc7",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939dc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install -q openai python-dotenv langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73905a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize language models\n",
    "llm = OpenAI(temperature=0.7)\n",
    "chat_model = ChatOpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38fb87",
   "metadata": {},
   "source": [
    "## 1. Zero-Shot Prompting\n",
    "\n",
    "Zero-shot prompting means asking the model to perform a task without examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_prompt(prompt_text):\n",
    "    \"\"\"Simple function for zero-shot prompting\"\"\"\n",
    "    response = chat_model([HumanMessage(content=prompt_text)])\n",
    "    return response.content\n",
    "\n",
    "# Example usage\n",
    "zero_shot_example = \"Explain quantum computing in simple terms.\"\n",
    "print(\"Prompt:\\n\", zero_shot_example)\n",
    "print(\"\\nResponse:\")\n",
    "print(zero_shot_prompt(zero_shot_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ae08b",
   "metadata": {},
   "source": [
    "## 2. Few-Shot Prompting\n",
    "\n",
    "Few-shot prompting provides examples in the prompt to help the model understand the expected pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81c46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot_prompt(examples, query):\n",
    "    \"\"\"Function for few-shot prompting with examples\"\"\"\n",
    "    # Format examples\n",
    "    formatted_examples = \"\\n\\n\".join(examples)\n",
    "    \n",
    "    # Construct full prompt\n",
    "    full_prompt = f\"{formatted_examples}\\n\\n{query}\"\n",
    "    \n",
    "    # Get response\n",
    "    response = chat_model([HumanMessage(content=full_prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Example usage for language translation\n",
    "translation_examples = [\n",
    "    \"English: The weather is nice today.\\nFrench: Il fait beau aujourd'hui.\",\n",
    "    \"English: What time is the meeting?\\nFrench: Quelle heure est la réunion?\"\n",
    "]\n",
    "\n",
    "translation_query = \"English: Where is the library?\\nFrench:\"\n",
    "\n",
    "print(\"Few-shot translation example:\")\n",
    "print(few_shot_prompt(translation_examples, translation_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35996954",
   "metadata": {},
   "source": [
    "## 3. Chain-of-Thought Prompting\n",
    "\n",
    "Chain-of-Thought (CoT) prompting guides the model to break down complex reasoning tasks step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_of_thought_prompt(problem):\n",
    "    \"\"\"Function for chain-of-thought prompting\"\"\"\n",
    "    prompt = f\"Think step-by-step to solve this problem: {problem}\"\n",
    "    response = chat_model([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Example math problem\n",
    "math_problem = \"\"\"If a train leaves at 2pm traveling at 60mph, and another train leaves \n",
    "at 3pm traveling at 90mph in the same direction, at what time will the second train \n",
    "catch up to the first train?\"\"\"\n",
    "\n",
    "print(\"Chain-of-Thought Example:\")\n",
    "print(chain_of_thought_prompt(math_problem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821ec9c",
   "metadata": {},
   "source": [
    "## 4. Role-Based Prompting\n",
    "\n",
    "Role-based prompting involves asking the model to take on a specific role or persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea3205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def role_prompt(role, task):\n",
    "    \"\"\"Function for role-based prompting\"\"\"\n",
    "    prompt = f\"You are a {role}. {task}\"\n",
    "    response = chat_model([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Example roles and tasks\n",
    "role_example = \"science educator for 8-year-olds\"\n",
    "task_example = \"Explain how rainbows are formed in an engaging and simple way.\"\n",
    "\n",
    "print(\"Role-Based Prompt Example:\")\n",
    "print(role_prompt(role_example, task_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf76daa",
   "metadata": {},
   "source": [
    "## 5. Template-Based Prompting\n",
    "\n",
    "Using templates to structure prompts consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3001c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_prompt(template, **kwargs):\n",
    "    \"\"\"Function for template-based prompting\"\"\"\n",
    "    prompt = template.format(**kwargs)\n",
    "    response = chat_model([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# Example template for generating content\n",
    "content_template = \"\"\"\n",
    "Topic: {topic}\n",
    "Audience: {audience}\n",
    "Tone: {tone}\n",
    "Format: {format}\n",
    "Length: {length}\n",
    "\n",
    "Please generate content based on the above specifications.\n",
    "\"\"\"\n",
    "\n",
    "# Example usage\n",
    "print(\"Template-Based Prompt Example:\")\n",
    "print(template_prompt(\n",
    "    content_template,\n",
    "    topic=\"Benefits of artificial intelligence in healthcare\",\n",
    "    audience=\"Medical professionals\",\n",
    "    tone=\"Informative and professional\",\n",
    "    format=\"Short blog post\",\n",
    "    length=\"300-400 words\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed8d6d",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "\n",
    "Let's create a simple evaluation framework to compare different prompting techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca06d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompts(task, techniques):\n",
    "    \"\"\"Evaluate different prompt techniques for a given task\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, prompt in techniques.items():\n",
    "        print(f\"Evaluating: {name}\")\n",
    "        response = chat_model([HumanMessage(content=prompt)])\n",
    "        results[name] = response.content\n",
    "        print(f\"\\nResponse:\\n{response.content}\\n{'='*50}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example task: Explain climate change\n",
    "task_description = \"Explain climate change\"\n",
    "\n",
    "prompt_techniques = {\n",
    "    \"Basic Prompt\": \"Explain climate change.\",\n",
    "    \"Role-Based\": \"You are a climate scientist. Explain climate change in a clear, factual manner.\",\n",
    "    \"Audience-Specific\": \"Explain climate change to a 12-year-old student.\",\n",
    "    \"Format-Specific\": \"Explain climate change in bullet points with the key causes, effects, and solutions.\",\n",
    "}\n",
    "\n",
    "# Uncomment to run evaluation\n",
    "# evaluate_prompts(task_description, prompt_techniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699240e3",
   "metadata": {},
   "source": [
    "## 7. Experiment with Your Own Prompts\n",
    "\n",
    "Use the cells below to experiment with your own prompt engineering techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537519ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom prompt here\n",
    "my_prompt = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to test your prompt\n",
    "# response = chat_model([HumanMessage(content=my_prompt)])\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca99dd",
   "metadata": {},
   "source": [
    "## 8. Prompt Engineering Best Practices\n",
    "\n",
    "Here are some best practices to keep in mind when engineering prompts:\n",
    "\n",
    "1. **Be specific and clear** about what you want\n",
    "2. **Provide context** to help the model understand the task\n",
    "3. **Break down complex tasks** into smaller steps\n",
    "4. **Use formatting** (bullet points, numbered lists, etc.) to structure your prompts\n",
    "5. **Specify constraints** (length, style, tone) when needed\n",
    "6. **Iterate and refine** based on the responses you get\n",
    "7. **Consider the model's limitations** and work within them\n",
    "8. **Test different approaches** to find what works best for your specific use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058c849",
   "metadata": {},
   "source": [
    "## 9. Guidelines for Prompting from DeepLearning.AI\n",
    "\n",
    "In this section, we'll practice two prompting principles and their related tactics in order to write effective prompts for large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0675701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative setup for OpenAI API\n",
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd902ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for using OpenAI's chat completions API\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "# For OpenAI library version 1.0.0 or newer, use this alternative:\n",
    "'''\n",
    "client = openai.OpenAI()\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4453e8e1",
   "metadata": {},
   "source": [
    "### Prompting Principles\n",
    "\n",
    "1. **Principle 1**: Write clear and specific instructions\n",
    "2. **Principle 2**: Give the model time to \"think\"\n",
    "\n",
    "Let's explore tactics for implementing these principles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9703a",
   "metadata": {},
   "source": [
    "### Tactic 1: Use delimiters to clearly indicate distinct parts of the input\n",
    "\n",
    "Delimiters can be anything like: ```, \"\"\", < >, <tag> </tag>, :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68348e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by \\ \n",
    "providing instructions that are as clear and \\ \n",
    "specific as you can possibly make them. \\ \n",
    "This will guide the model towards the desired output, \\ \n",
    "and reduce the chances of receiving irrelevant \\ \n",
    "or incorrect responses. Don't confuse writing a \\ \n",
    "clear prompt with writing a short prompt. \\ \n",
    "In many cases, longer prompts provide more clarity \\ \n",
    "and context for the model, which can lead to \\ \n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\ \n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083a84d",
   "metadata": {},
   "source": [
    "### Tactic 2: Ask for a structured output\n",
    "\n",
    "Requesting specific formats like JSON or HTML can help structure the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Generate a list of three made-up book titles along \\ \n",
    "with their authors and genres. \n",
    "Provide them in JSON format with the following keys: \n",
    "book_id, title, author, genre.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7772bf3",
   "metadata": {},
   "source": [
    "### Tactic 3: Ask the model to check whether conditions are satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc3d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = f\"\"\"\n",
    "Making a cup of tea is easy! First, you need to get some \\ \n",
    "water boiling. While that's happening, \\ \n",
    "grab a cup and put a tea bag in it. Once the water is \\ \n",
    "hot enough, just pour it over the tea bag. \\ \n",
    "Let it sit for a bit so the tea can steep. After a \\ \n",
    "few minutes, take out the tea bag. If you \\ \n",
    "like, you can add some sugar or milk to taste. \\ \n",
    "And that's it! You've got yourself a delicious \\ \n",
    "cup of tea to enjoy.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\ \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 1:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeaa160",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = f\"\"\"\n",
    "The sun is shining brightly today, and the birds are \\\n",
    "singinging. It's a beautiful day to go for a \\ \n",
    "walk in the park. The flowers are blooming, and the \\ \n",
    "trees are swaying gently in the breeze. People \\ \n",
    "are out and about, enjoying the lovely weather. \\ \n",
    "Some are having picnics, while others are playing \\ \n",
    "games or simply relaxing on the grass. It's a \\ \n",
    "perfect day to spend time outdoors and appreciate the \\ \n",
    "beauty of nature.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes. \n",
    "If it contains a sequence of instructions, \\ \n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\ \n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 2:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e8505",
   "metadata": {},
   "source": [
    "### Tactic 4: \"Few-shot\" prompting\n",
    "\n",
    "Providing examples of successful executions of the task you want performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest \\ \n",
    "valley flows from a modest spring; the \\ \n",
    "grandest symphony originates from a single note; \\ \n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about resilience.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f0137",
   "metadata": {},
   "source": [
    "## Principle 2: Give the model time to \"think\"\n",
    "\n",
    "If a model makes reasoning errors by rushing to an incorrect conclusion, you should try to give it time to work through the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ab0f6",
   "metadata": {},
   "source": [
    "### Tactic 1: Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db16168",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "In a charming village, siblings Jack and Jill set out on \\ \n",
    "a quest to fetch water from a hilltop \\ \n",
    "well. As they climbed, singing joyfully, misfortune \\ \n",
    "struck—Jack tripped on a stone and tumbled \\ \n",
    "down the hill, with Jill following suit. \\ \n",
    "Though slightly battered, the pair returned home to \\ \n",
    "comforting embraces. Despite the mishap, \\ \n",
    "their adventurous spirits remained undimmed, and they \\ \n",
    "continued exploring with delight.\n",
    "\"\"\"\n",
    "# example 1\n",
    "prompt_1 = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple \\\n",
    "backticks with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the following \\\n",
    "keys: french_summary, num_names.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "\n",
    "Text:\n",
    "```{text}```\n",
    "\"\"\"\n",
    "response = get_completion(prompt_1)\n",
    "print(\"Completion for prompt 1:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3a544",
   "metadata": {},
   "source": [
    "### Asking for output in a specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c43bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = f\"\"\"\n",
    "Your task is to perform the following actions: \n",
    "1 - Summarize the following text delimited by \n",
    "  <> with 1 sentence.\n",
    "2 - Translate the summary into French.\n",
    "3 - List each name in the French summary.\n",
    "4 - Output a json object that contains the \n",
    "  following keys: french_summary, num_names.\n",
    "\n",
    "Use the following format:\n",
    "Text: <text to summarize>\n",
    "Summary: <summary>\n",
    "Translation: <summary translation>\n",
    "Names: <list of names in summary>\n",
    "Output JSON: <json with summary and num_names>\n",
    "\n",
    "Text: <{text}>\n",
    "\"\"\"\n",
    "response = get_completion(prompt_2)\n",
    "print(\"\\nCompletion for prompt 2:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf727929",
   "metadata": {},
   "source": [
    "### Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \\\n",
    " help working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\ \n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d6e7d",
   "metadata": {},
   "source": [
    "Note that the student's solution is actually not correct. We can fix this by instructing the model to work out its own solution first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46f2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to determine if the student's solution \\\n",
    "is correct or not.\n",
    "To solve the problem do the following:\n",
    "- First, work out your own solution to the problem including the final total. \n",
    "- Then compare your solution to the student's solution \\ \n",
    "and evaluate if the student's solution is correct or not. \n",
    "Don't decide if the student's solution is correct until \n",
    "you have done the problem yourself.\n",
    "\n",
    "Use the following format:\n",
    "Question:\n",
    "```\n",
    "question here\n",
    "```\n",
    "Student's solution:\n",
    "```\n",
    "student's solution here\n",
    "```\n",
    "Actual solution:\n",
    "```\n",
    "steps to work out the solution and your solution here\n",
    "```\n",
    "Is the student's solution the same as actual solution \\\n",
    "just calculated:\n",
    "```\n",
    "yes or no\n",
    "```\n",
    "Student grade:\n",
    "```\n",
    "correct or incorrect\n",
    "```\n",
    "\n",
    "Question:\n",
    "```\n",
    "I'm building a solar power installation and I need help \\\n",
    "working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\\n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \\\n",
    "as a function of the number of square feet.\n",
    "``` \n",
    "Student's solution:\n",
    "```\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "```\n",
    "Actual solution:\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333bdfc",
   "metadata": {},
   "source": [
    "## Model Limitations: Hallucinations\n",
    "\n",
    "Large language models can generate outputs that sound convincing but are actually made up or incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaab754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the model making up information about a real company with a fictitious product\n",
    "prompt = f\"\"\"\n",
    "Tell me about AeroGlide UltraSlim Smart Toothbrush by Boie\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c175d70a",
   "metadata": {},
   "source": [
    "## Notes on using the OpenAI API outside of this classroom\n",
    "\n",
    "### Installing the OpenAI Python library\n",
    "```python\n",
    "!pip install openai\n",
    "```\n",
    "\n",
    "### Setting up your API key\n",
    "The library needs to be configured with your account's secret key, which is available on the OpenAI website.\n",
    "\n",
    "You can either set it as the `OPENAI_API_KEY` environment variable before using the library:\n",
    "```bash\n",
    "export OPENAI_API_KEY='sk-...'\n",
    "```\n",
    "\n",
    "Or, set `openai.api_key` directly:\n",
    "```python\n",
    "import openai\n",
    "openai.api_key = \"sk-...\"\n",
    "```\n",
    "\n",
    "### A note about the backslash\n",
    "In the examples above, we're using a backslash `\\` to make the text fit on the screen without inserting newline characters. GPT models aren't significantly affected by newline characters, but when working with LLMs in general, you may want to consider whether newline characters in your prompt could affect the model's performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
